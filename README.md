# Team_mid_hackathon

Hackathon Problem Statement: Improving Annotation Quality via Mid-Annotation LLM Feedback

High-stakes domains like finance suffer from annotation errors due to cognitive load and lack of real-time support. While LLMs help in pre- and post-annotation, mid-annotation feedback remains underutilized.
We propose a real-time LLM feedback mechanism that monitors annotation decisions, flags inconsistencies, provides contextual insights (e.g., historical term usage), and queries annotator uncertainties, while leaving final control with experts.
The system integrates into existing workflows (e.g., Round-trip Annotator) to reduce ambiguity and annotation drift.
Goal: Build a prototype mid-annotation feedback system and evaluate its effect on:

Inter-Annotator Agreement (Cohen’s/Fleiss’ kappa)

Annotation Quality (F1 vs. gold standard)

Annotation Speed (tokens/minute)
A comparative study with standard workflows will validate impact.
This project bridges manual expertise and AI assistance, promising faster, more accurate, and consistent annotations for regulated domains.

